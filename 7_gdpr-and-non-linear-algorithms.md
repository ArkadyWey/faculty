# Day 7 
Jul Lussier-Craig 

## GDPR 

### Scope 
- Data processed in UK 

### Personal data 
- Info that allows someone to be identified 
- Examples include passport, ID, etc... 

### Pseudonymised and anonamised data 
- Anonmymised : Data where the subject is not identifiable anymore  
- Pseudonymised : Can't tell who the person is without more info 

### Data processing principles 
- Lawfullness : Data that has been processes in a transparent way - need to tell them what you're doing. 
- Data limiation : Only connect data that you need and you ahve to tell them what you are using it for and can't use it for something else later. Hard for faculty because qwe like to take ddata and then use it in interesting way. 
- Data minimisation : Do not hoover up large amounts of data that we don't need because makes more vulnerble to things like hacks. Interesting for us because we need to try and work out a priori what data we need before the project gets going. 
- Accuracy : Data needs to be accurate and kept up to date because if we make decisions based on false data then it could be bad. 
- Storage limitation : Only keep data for as long as you need it. 
- Security and integriity : Don't share it without permission. 

### Lawful 
- Principle 1 : Data needs to be processed lawfully, fairly, and transparently. 
- Article 6 : 

## 2. Non-linear Algorithms 
Nicholas Jennings 

### Motivation  
- We want to be able to classify non-linearly separable data points. 
- Parametric algorithms : Learning a set of parameters that define a pre-defined model (function) e.g. neural networks/linear regression
- Non-parametric algorithms : No assumptions about the parameters to be learned / not writing out a function given by parameters e.g. k-nearest neighbours 

### KNNs : K Nearest Neighbours 
- Look at a data point 
- Look at the nearest neighbours  
- Classiify the same as neighbours 

Thigns to consider
- What should k be? : Trade-off in bias invariance. If k is small then we might pick up one random anomaly that is very close, rather than the general trend. IF k is too high, then you're just biased to what there are the most of in the data set. In practice we try  a bunch of k and see where the elbow is.
- What's the distance metric? : Manhattan / Euclidean / Chebyshev

#### Cons 
- Struggles with small train sets with many feture 
- Does not scale well with training data size 
- Treats every dimension feature equally 
- Preprocessing is crucial (dimension reduction / feature selection)\

#### Complexity 
Training: O(1) time, O(n) space since I load n data points 
Inference O(dn) time, O(k) space  


### Decision tress 
- Split basedd on a series of binary decisions 

Questions 
- How do we decide which feature to split on? : If we haven't really split the data any further then that is a  bad split. So we need to measure the information gain from using a certain split. For this we use the Kullback-Leiibler Divergence. This is small when the new distribution 
- How do we decide what the value we split by should be?

Advantages 
- No preprocessing : entropy of the split is not dependent on the dimensions of the data.
- Feature importanc : More important features appear at the top of the tree since they split the data better  
- Integpretability - stakeholders tendd to understand it 

Disadvantages 
- Splits orthogonal to features so iif data has different shape it might struggle. 
- Unstable : small change in data gives biig change in tree 
- Low accuracy : Doesn't work that well. Use random forest instead. 

#### Complexity 
Training
- Time : O(m.n.log(n)) m=features, n=data points 
- Space : O(depth)

Inference
Time : O(depth)
Space : O(1) 

## 3. Documentation  
Will Massey 

### Why do we need documentation?
- Spend a lot more time reading than writing 
- Lot everything you write is obvious
- YOur code may have a longer life than you think
- You will forget it yourself

### What is documentation?
- README 
- Inline 
- Docstrings 
- Tests 
- Commits and PRs 
- Issues 
- Diagrams 
- Supportng documentation 
- Pydantic
- Type annotations
- Release notes

### Principles  
- Explain why not what 
- Keep it short 
- Think about audience  
- Don't leave it until the end of the project 
- It's an inivestment : it pays back! 
 
### Types of documentation  

Code comments
- Comment after code
- Comment on a single line 

Docstrings
- PEP 

Type annotations 
- Python is dynamically typed  
- But we use type suggestions to annotate python code  
- Tools like mypy enforce type suggesting
- from typing import Optional, Union

README 
- Contain everything to get started with the project 

.md 
- Used for read me
- Used for merge requests  

Tests 
- Help to explain how your fcode should behave 
- Most time is spent reading 
 
Autogenerated docs 
- Swagger 
- Sphinx

My preference - most important thing fiirst
- Good code 
- Short docstringis on every function 
- Tests 
- Basic ReadMe
- Type annotations  
...
- A massive README
... 
- DBT 

Tools 
- MYPy - turn on ignore missing imoports 
- Autodocstring 





